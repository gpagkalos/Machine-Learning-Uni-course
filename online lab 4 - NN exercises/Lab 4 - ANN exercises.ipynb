{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Exercises\n",
    "\n",
    "\n",
    "In particular, in this homework we consider neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "- A **linear module** that implements a linear transformation: $z_{j}=\\left(\\sum_{i=1}^{m} x_{i} w_{i, j}\\right)+w_{0 j}$ specified by a weight matrix $W$ and a bias vector $W_{0}$. The output is $\\left[z_{1}, \\ldots, z_{n}\\right]^{T}$.\n",
    "\n",
    "- An **activation module** that applies an activation function to the outputs of the linear module for some activation function $f$, such as `Tanh` or `ReLU` in the hidden layers or `Softmax` (see below) at the output layer. We write the output as: : $\\left[f\\left(z_{1}\\right), \\ldots, f\\left(z_{m}\\right)\\right]^{T}$, although technically, for some activation functions such as `softmax`, each output will depend on all the $z_{i}$, not just one.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "- Inputs to the network are $x_{1}, \\ldots, x_{d}$.\n",
    "- Number of layers is $L$\n",
    "- There are $m^{l}$ inputs to layer $l$\n",
    "- There are $n^{l}=m^{l+1}$ outputs from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^{l}$, an $m^{l} \\times n^{l}$ matrix, and the bias vector (offset) is $W_{0}^{l}$, an $n^{l} \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as pre-activation values and denoted $z^{l}$\n",
    "- The activation function at layer $l$ is $f^{l}(\\cdot)$\n",
    "- Layer $l$ activations are $a^{l}=\\left[f^{l}\\left(z_{1}^{l}\\right), \\ldots, f^{l}\\left(z_{n^{l}}^{l}\\right)\\right]^{T}$\n",
    "- The output of the network is the values $a^{L}=\\left[f^{L}\\left(z_{1}^{L}\\right), \\ldots, f^{L}\\left(z_{n^{L}}^{L}\\right)\\right]^{T}$\n",
    "- Loss function $\\operatorname{Loss}(a, y)$ measures the loss of output values $a$ when the target is $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and output activations: classification\n",
    "\n",
    "When doing classification, it's natural to think of the output values as being discrete: $+1$ and $-1$. But it is generally difficult to use optimization-based methods without somehow thinking of the outputs as being continuous (even though you will have to discretize when it's time to make a prediction.\n",
    "\n",
    "###  Hinge loss, linear activation\n",
    "\n",
    "When we looked at the SVM objective for classification, we did this:\n",
    "- Defined the output space to be $\\mathbb{R}$\n",
    "- Developed the hinge loss function\n",
    "$$\n",
    "\\operatorname{Loss}(a, y)=L_{h}(y a)= \\begin{cases}0 & \\text { if } y a>1 \\\\ 1-y a & \\text { otherwise }\\end{cases}\n",
    "$$\n",
    "where $a$ is the continuous output (we're using $a$ here to be consistent with the neural network terminology of activation) and $y$ is the desired/target output\n",
    "\n",
    "- Tried to find parameters $\\theta$ of our model to minimize loss summed over the training data\n",
    "\n",
    "Consider a single \"neuron\" with a linear activation function; that is, where $a_{1}^{L}=\\sum_{k} w_{k, 1}^{L} x_{k}+w_{0,1}^{L}$. In this case, we have $L=1$ and $f^{L}(z)=z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Write a short program to compute the gradient of the loss function with respect to the weight vector (not the bias): $\\nabla_{w^{L}} \\operatorname{Loss}\\left(a_{1}^{L}, y\\right)$ when $\\operatorname{Loss}(a, y)=L_{h}(y a)$.\n",
    "- $\\mathrm{x}$ is a column vector\n",
    "- y is a number, a label\n",
    "- $a$ is a number, an activation\n",
    "It should return a column vector.\n",
    "\n",
    "        def hinge_loss_grad (x, y, a):\n",
    "            pass\n",
    "\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_grad (x, y, a): \n",
    "    if (y.multiply(a)>1):\n",
    "        return(0)\n",
    "    else:\n",
    "        return(-y*sum(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss, sigmoidal activation\n",
    "\n",
    "Another way to make the output for a classifier continuous is to make it be in the range $(0,1)$, which admits the interpretation of being the predicted probability that the example is positive. A convenient way to make the activation of a unit be in the range $(0,1)$ is to use a sigmoid function:\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "The figure below shows a sigmoid activation function on the left, with the rectified linear (ReLU) activation function on the right for comparison.\n",
    "\n",
    "![](sig_relu_v1.png)\n",
    "\n",
    "\\begin{exercise}\n",
    "What is an expression for the derivative of the sigmoid with respect to z, expressed as a function of z, its input?\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:\n",
    "$$\\frac{d\\sigma(z)}{dz}=\\frac{d}{dz}\\left(\\frac{1}{1+e^{-z}}\\right)= -(1+e^{-z})^{-2}\\cdot(-e^{-z})=\\frac{e^{-z}}{(1+e^{-z})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "What is an expression for the derivative of the sigmoid with respect to $z$, but this time expressed as a function of $o=$ $\\sigma(z)$, its output?\n",
    "Hint: Think about the expression $1-\\frac{1}{1+e^{-z}}$. (Here https://www.mathsisfun.com/ contains a review of computing derivatives.)\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:\n",
    "$$\\frac{d\\sigma(z)}{dz}=\\frac{e^{-z}}{(1+e^{-z})^{2}}=\\frac{e^{-z}+1-1}{(1+e^{-z})^{2}}=\\frac{e^{-z}+1}{(1+e^{-z})^{2}}-\\frac{1}{(1+e^{-z})^{2}}=\\frac{1}{(1+e^{-z})}-\\frac{\\sigma(z)}{(1+e^{-z})}=\\frac{1}{(1+e^{-z})}(1-\\sigma(z))=\\sigma(z)(1-\\sigma(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function for predicting probabilities \n",
    "\n",
    "We will consider positive points to have label $+1$, and negative points to have label 0 .\n",
    "\n",
    "We need a loss function that works well when we are predicting probabilities. A good choice is to ask what probability is assigned to the correct label. We will interpret the value outputted by our classifier as the probability that the example is positive. So, if the output value is $a$ and the true label is $+1$, then the probability assigned to the true label is $a$; on the other hand, if the true label is 0 , then the probability assigned to the true label is $1-a$. Because we actually will be interested in the probability of the predictions on the whole data set, we'd want to choose weights to maximize\n",
    "$$\n",
    "\\prod_{t} P\\left(a^{(t)}, y^{(t)}\\right)\n",
    "$$\n",
    "where $P\\left(a^{(t)}, y^{(t)}\\right)$ is the probability that the network predicts the correct label for data point $(t)$.\n",
    "Using a notational trick (which turns an if expression into a product)  we can write the probability $P(a, y)$ as\n",
    "$$\n",
    "P(a, y)=a^{y}(1-a)^{(1-y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "\n",
    "What is the value of  $P(a, y)$  when  $y=0$  and $y=1$?\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:\n",
    "\n",
    "for $\\space y=0 , \\space P(a, 0)=a^{0}(1-a)^{1}=(1-a)$ \n",
    "\n",
    "for $\\space y=1 , \\space P(a, 1)=a^{1}(1-a)^{0}=a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Find a simplified expression for $\\log P(a, y)$ that does not use exponentiation. Note that we refer to the natural logarithm $\\ln$ as $\\log$ throughout this assignment.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:\n",
    "$\\log\\left( P(a, y)\\right) = \\log \\left(a^{y}(1-a)^{(1-y)}\\right)= y\\log(a)+(1-y)\\log(1-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, because log is a monotonic function, the same weights that maximize the product of the probabilities will minimize the negative log likelihood (\"likelihood\" is the same as probability; we just use that name here because the phrase is an idiom in machine learning, abbreviated NLL):\n",
    "$$\n",
    "\\operatorname{Loss}(a, y)=N L L(a, y)=-y \\log a-(1-y) \\log (1-a) .\n",
    "$$\n",
    "Our objective function (over our $n$ data points) will then be\n",
    "$$\n",
    "\\sum N L L\\left(a^{(t)}, y^{(t)}\\right)=-\\sum_{t=1}^{n}\\left[y^{(t)} \\log a^{(t)}+\\left(1-y^{(t)}\\right) \\log \\left(1-a^{(t)}\\right)\\right] .\n",
    "$$\n",
    "Remember that $a^{(t)}$ is our model's output for training example $t$, and $y^{(t)}$ is the true label (+ 1 or 0 ).\n",
    "Now, we can think about a single unit with a sigmoidal activation function, trained to minimize NLL. So, $a_{1}^{L}=$ $\\sigma\\left(\\sum_{k} w_{k, 1}^{L} x_{k}+w_{0,1}^{L}\\right)$. In this case, we have $L=1$.\n",
    "\n",
    "\\begin{exercise}\n",
    "Write a formula for the gradient of the NLL with respect to the first weight, $\\nabla_{w_{1,1}^{L}} N L L\\left(a_{1}^{L}, y\\right)$, for a single training example. Hint: consider using the chain rule; the final answer (expression) is very short.\n",
    "\n",
    "Write an expression of the gradient in terms of $x_1, a_1$, and $y$\n",
    "\\end{exercise}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer: \n",
    "\n",
    "$$\\nabla_{w_{1,1}^{L}} N L L\\left(a_{1}^{L}, y\\right)=\\nabla_{w_{1,1}^{L}}\\left(-y \\log a_1-(1-y) \\log (1-a_1) \\right)= \\nabla_{w_{1,1}^{L}}\\left(-y \\log \\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right)-(1-y) \\log (1-\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right)) \\right)=-y\\frac{1}{\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right)}\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right)\\left(1-\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right) \\right)x_1+(1-y)\\frac{1}{\\left(1-\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right) \\right)})\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right)\\left(1-\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right) \\right)x_1= -y(1-\\sigma\\left( w_{1, 1} x_{1}+w_{0,1}\\right))x_1+(1-y) \\sigma \\left( w_{1, 1} x_{1}+w_{0,1}\\right)x_1= -y(1-a_1)x_1+(1-y)a_1x_1=x_1(a_1 - y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Write a formula for the gradient of the NLL with respect to the full weight vector, $\\nabla_{W^{L}} N L L\\left(a_{1}^{L}, y\\right)$, for a single training example.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using matrix notation:  $X(A-Y)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "What if we needed to classify homework problems into three categories: enlightening, boring, impossible? We can do this by using a \"one-hot\" encoding on the output, and using three output units with what is called a \"softmax\" (SM) activation module. It's not a typical activation module, since it takes in all $n_{L}$ pre-activation values $z_{j}^{L}$ in $\\mathbb{R}$ and returns $n_{L}$ output values $a_{j}^{L} \\in$ $[0,1]$ such that $\\sum_{j} a_{j}^{L}=1$. This can be interpreted as representing a probability distribution over the possible categories.\n",
    "The individual entries are computed as\n",
    "$$\n",
    "a_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{n^{L}} e^{z_{k}}}\n",
    "$$\n",
    "We'll describe the relationship of the vector $a$ on the vector $z$ as\n",
    "$$\n",
    "a=\\operatorname{SM}(z)\n",
    "$$\n",
    "The network below shows a one-layer network with a linear module followed by a softmax activation module.\n",
    "\n",
    "![](softmax.png)\n",
    "\n",
    "\\begin{exercise}\n",
    "What probability distribution over the categories is represented by $z^{L}=[-1,0,1]^{T}$ ?\n",
    "Enter a distribution (a list of three numbers adding up to 1) for the three categories. Your answers should be numeric (please enter numbers, and do not use the symbol $e$ ):\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer: \n",
    "\n",
    "$$ a = [a_{1},a_{2},a_{3}]=[\\frac{e^{z_{1}}}{\\sum_{k=1}^{3} e^{z_{k}}},\\frac{e^{z_{2}}}{\\sum_{k=1}^{3} e^{z_{k}}},\\frac{e^{z_{3}}}{\\sum_{k=1}^{3} e^{z_{k}}}] = [\\frac{e^{-1}}{e^{-1}+e^{0}+e^{1}},\\frac{e^{0}}{e^{-1}+e^{0}+e^{1}},\\frac{e^{1}}{e^{-1}+e^{0}+e^{1}}]=[\\frac{0.367}{4.086},\\frac{1}{4.086},\\frac{2.718}{4.086}] = [0.09,0.66,0.24]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a loss function $\\operatorname{Loss}(a, y)$ where $a$ is a discrete probability distribution and $y$ is a one-hot vector encoding of a single output value. It makes sense to use negative log likelihood as a loss function for the same reasons as before. So, we'll just extend our definition of NLL from earlier:\n",
    "$$\n",
    "N L L(a, y)=-\\sum_{j=1}^{n^{L}} y_{j} \\ln a_{j}^{L}\n",
    "$$\n",
    "Note that the above expression is for multi-classes (number of class $>2$ ). For two-classes, the expression reduce to what you have seen before.\n",
    "\n",
    "\\begin{exercise}\n",
    "If $a=[.3, .5, .2]^{T}$ and $y=[0,0,1]^{T}$, what is $N L L(a, y)$ ?\n",
    "Enter an expression involving `log(.)` (for natural log) and constants.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer: \n",
    "$$\n",
    "N L L([.3, .5, .2],[0,0,1])=-\\sum_{j=1}^{3} y_{j} \\ln a_{j}= -(0\\cdot \\ln(0.3) + 0\\cdot \\ln(0.5)+ 1\\cdot \\ln(0.2)= \\ln(0.2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can think about a single layer with a softmax activation module, trained to minimize NLL. The pre-activation values (the output of the linear module) are:\n",
    "$$\n",
    "z_{j}^{L}=\\sum_{k} w_{k, j}^{L} x_{k}+w_{0, j}^{L}\n",
    "$$\n",
    "and $a^{L}=S M\\left(z^{L}\\right)$\n",
    "To do gradient descent, we need to know $\\frac{\\partial}{\\partial w_{k, j}^{L}} N L L\\left(a^{L}, y\\right)$. We'll reveal the secret (that you might guess from Problem 1 ) that it has an awesome form! (Please consider deriving this, for fun and satisfaction!)\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{k, j}^{L}} N L L\\left(a^{L}, y\\right)=x_{k}\\left(a_{j}^{L}-y_{j}\\right)\n",
    "$$\n",
    "And of course, it's easy to compute the whole matrix of these derivatives, $\\nabla_{W^{L}} N L L\\left(a^{L}, y\\right)$, in one quick matrix computation.\n",
    "2.C) Suppose we have two input units and three possible output values, and the weight matrix $W^{L}$ is\n",
    "$$\n",
    "W^{L}=\\left[\\begin{array}{ccc}\n",
    "1 & -1 & -2 \\\\\n",
    "-1 & 2 & 1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "or in Python form: \n",
    "    \n",
    "        w = np.array([[1, -1, -2], [-1, 2, 1]])\n",
    "\n",
    "\n",
    "Assume the biases are zero, the input $x=[1,1]^{T}$ (e.g., `x = np.array([[1, 1]]).T`, and the target output $y=[0,1,0]^{T}$ (e.g., `y = np.array([[0, 1, 0]]).T`). \n",
    "\n",
    "\n",
    "\\begin{exercise}\n",
    "What is the matrix $\\nabla_{W^{L}} N L L\\left(a^{L}, y\\right)$ ? Hint: You might want to solve using Python and numpy.\n",
    "Enter the matrix as a list of lists, one list for each row of the matrix. Please enter values with a precision of three decimal points.\n",
    "\\end{exercise}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1 -1]]\n",
      "[[0.245 0.665 0.09 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.245, -0.335,  0.09 ],\n",
       "       [ 0.245, -0.335,  0.09 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return (np.around(e_x / e_x.sum(),3))\n",
    "\n",
    "w = np.array([[1, -1, -2], [-1, 2, 1]])\n",
    "x = np.array([[1, 1]]).T\n",
    "y = np.array([[0, 1, 0]])\n",
    "\n",
    "z = np.dot(w.T,x).T\n",
    "print(z)\n",
    "a=softmax(z)\n",
    "print(a)\n",
    "NLL=np.dot(x,(a-y))\n",
    "NLL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "What is the predicted probability that xx is in class 1, before any gradient updates? (Assume we have classes 0, 1, and 2.). Enter a number and justify your answer.\n",
    "\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:\n",
    "\n",
    "vector 'a' describes the predicted probability of x belonging to each class. For x in class 1 the probability is 0.665 (most probable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Using step size $0.5$, what is $W^{L}$ after one gradient update step?\n",
    "Enter the matrix as a list of lists, one list for each row of the matrix. Please enter values with a precision of three decimal points.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8775 -0.8325 -2.045 ]\n",
      " [-1.1225  2.1675  0.955 ]]\n"
     ]
    }
   ],
   "source": [
    "Wnew=w-0.5*NLL\n",
    "print(Wnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "What is the predicted probability that x is in class 1, given the new weight matrix?\n",
    "\n",
    "Enter a number and justify your answer.\n",
    "\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.245  1.335 -1.09 ]]\n",
      "[[0.159 0.773 0.068]]\n"
     ]
    }
   ],
   "source": [
    "z = np.dot(Wnew.T,x).T\n",
    "print(z)\n",
    "a=softmax(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the new weight matrix, the probability of x being class 1 is now 0.773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "In this problem we will analyze a simple neural network to understand its classification properties. \n",
    "\n",
    "Consider the neural network given in the figure below, with ReLU activation functions ( $f^{1}$ in the figure) on all hidden neurons, and softmax activation ( $f^{2}$ in the figure) for the output layer, resulting in softmax outputs ( $a_{1}^{2}$ and $a_{2}^{2}$ in the figure).\n",
    "\n",
    "![](nnet.png)\n",
    "\n",
    "Given an input $x=\\left[x_{1}, x_{2}\\right]^{T}$, the hidden units in the network are activated in stages as described by the following equations:\n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "z_{1}^{1}=x_{1} w_{1,1}^{1}+x_{2} w_{2,1}^{1}+w_{0,1}^{1} & a_{1}^{1}=\\max \\left\\{z_{1}^{1}, 0\\right\\} \\\\\n",
    "z_{2}^{1}=x_{1} w_{1,2}^{1}+x_{2} w_{2,2}^{1}+w_{0,2}^{1} & a_{2}^{1}=\\max \\left\\{z_{2}^{1}, 0\\right\\} \\\\\n",
    "z_{3}^{1}=x_{1} w_{1,3}^{1}+x_{2} w_{2,3}^{1}+w_{0,3}^{1} & a_{3}^{1}=\\max \\left\\{z_{3}^{1}, 0\\right\\} \\\\\n",
    "z_{4}^{1}=x_{1} w_{1,4}^{1}+x_{2} w_{2,4}^{1}+w_{0,4}^{1} & a_{4}^{1}=\\max \\left\\{z_{4}^{1}, 0\\right\\} \\\\\n",
    "& \\\\\n",
    "z_{1}^{2}=a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2} \\\\\n",
    "z_{2}^{2}=a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The final output of the network is obtained by applying the softmax function to the last hidden layer,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{1}^{2} &=\\frac{e^{z_{1}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} \\\\\n",
    "a_{2}^{2} &=\\frac{e^{z_{2}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "In this problem, we will consider the following setting of parameters:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "{\\left[\\begin{array}{cccc}\n",
    "w_{1,1}^{1} & w_{1,2}^{1} & w_{1,3}^{1} & w_{1,4}^{1} \\\\\n",
    "w_{2,1}^{1} & w_{2,2}^{1} & w_{2,3}^{1} & w_{2,4}^{1}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "1 & 0 & -1 & 0 \\\\\n",
    "0 & 1 & 0 & -1\n",
    "\\end{array}\\right], \\quad\\left[\\begin{array}{c}\n",
    "w_{0,1}^{1} \\\\\n",
    "w_{0,2}^{1} \\\\\n",
    "w_{0,3}^{1} \\\\\n",
    "w_{0,4}^{1}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "-1\n",
    "\\end{array}\\right],} \\\\\n",
    "{\\left[\\begin{array}{ll}\n",
    "w_{1,1}^{2} & w_{1,2}^{2} \\\\\n",
    "w_{2,1}^{2} & w_{2,2}^{2} \\\\\n",
    "w_{3,1}^{2} & w_{3,2}^{2} \\\\\n",
    "w_{4,1}^{2} & w_{4,2}^{2}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "1 & -1 \\\\\n",
    "1 & -1 \\\\\n",
    "1 & -1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right], \\quad\\left[\\begin{array}{c}\n",
    "w_{0,1}^{2} \\\\\n",
    "w_{0,2}^{2}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "0 \\\\\n",
    "2\n",
    "\\end{array}\\right]}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "Consider the input $x_{1}=3, x_{2}=14$.\n",
    "What are the outputs of the hidden units, $\\left(f^{1}\\left(z_{1}^{1}\\right), f^{1}\\left(z_{2}^{1}\\right), f^{1}\\left(z_{3}^{1}\\right), f^{1}\\left(z_{4}^{1}\\right)\\right)$.\n",
    "Enter a Python list of 4 numbers.\n",
    "\n",
    "\\end{exercise}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  13  -4 -15]]\n",
      "answer: [[ 2 13  0  0]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return (np.around(e_x / e_x.sum(),3))\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(0, a)\n",
    "\n",
    "w1= np.array([[1, 0, -1, 0], [0, 1, 0, -1]])\n",
    "b1=np.array([[-1], [-1], [-1], [-1]])\n",
    "w2=np.array([[1, -1],[1, -1],[1, -1],[1, -1]])\n",
    "b2=np.array([[0], [2]])\n",
    "x = np.array([[3, 14]]).T\n",
    "#print(w1)\n",
    "#print(b1)\n",
    "#print(w2)\n",
    "#print(b2)\n",
    "z1 = (np.dot(w1.T,x)+b1).T\n",
    "print(z1)\n",
    "a1=relu(z1)\n",
    "print('answer:',a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "What is the final output $\\left(a_{1}^{2}, a_{2}^{2}\\right)$ of the network? Enter a Python list of 2 numbers.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 15 -13]]\n",
      "answer: [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "z2 = (np.dot(w2.T,a1.T)+b2).T\n",
    "print(z2)\n",
    "a2=softmax(z2)\n",
    "print('answer:',a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit decision boundaries\n",
    "Let's characterize the decision boundaries in $x$-space, corresponding to the four hidden units. These are the regions where the input to the units $z_{1}^{1}, z_{2}^{1}, z_{3}^{1}, z_{4}^{1}$ are exactly zero.\n",
    "\n",
    "Hint: You should describe a diagram of the decision boundaries for each unit in the $x$-space and label the sides of the boundaries with 0 and $+$ to indicate whether the unit's output would be exactly 0 or positive, respectively. (The diagram should be a 2D plot with $x_{1}$ and $x_{2}$ on each axis, with lines for $z_{1}^{1}=0, z_{2}^{1}=0, z_{3}^{1}=0, z_{4}^{1}=0$.)\n",
    "\n",
    "\\begin{exercise}\n",
    "What is the shape of the decision boundary for a single unit?\n",
    "\\end{exercise}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot([1, 1],[-10, 10])\n",
    "plt.plot([-10, 10],[1, 1])\n",
    "plt.plot([-10, 10],[-1, -1])\n",
    "plt.plot([-1, -1],[-10, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{example}\n",
    "Consider the following input vectors: $x^{(1)}=[0.5,0.5]^{T}, x^{(2)}=[0,2]^{T}, x^{(3)}=[-3,0.5]^{T}$. Enter a matrix where each column represents the outputs of the hidden units $\\left(f\\left(z_{1}^{1}\\right), \\ldots, f\\left(z_{4}^{1}\\right)\\right)$ for each of the input vectors. You can use your diagram of decision boundaries.\n",
    "Enter a Python list of lists where each list is a row of the matrix.\n",
    "\n",
    "\\end{example}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5 -0.5 -1.5 -1.5]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[-1  1 -1 -3]]\n",
      "[[0 1 0 0]]\n",
      "[[-4.  -0.5  2.  -1.5]]\n",
      "[[0. 0. 2. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 2., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1= np.array([[1, 0, -1, 0], [0, 1, 0, -1]])\n",
    "b1=np.array([[-1], [-1], [-1], [-1]])\n",
    "x = [np.array([[0.5, 0.5]]).T,np.array([[0, 2]]).T,np.array([[-3, 0.5]]).T]\n",
    "result =np.array([[]])\n",
    "for i,xi in enumerate(x):\n",
    "    #print(w1)\n",
    "    #print(b1)\n",
    "    #print(w2)\n",
    "    #print(b2)\n",
    "    z1 = (np.dot(w1.T,xi)+b1).T\n",
    "    print(z1)\n",
    "    a1=relu(z1)\n",
    "    print(a1)\n",
    "    if(i==0):\n",
    "        result=a1\n",
    "    else:\n",
    "        result = np.vstack([result, a1])     \n",
    "        \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network outputs\n",
    "\n",
    "In our network above, the output layer with two softmax units is used to classify into one of two classes. For class 1 , the first unit's output should be larger than the other unit's output, and for class 2, the second unit's output should be larger. This generalizes nicely to $k$ classes by using $k$ output units.\n",
    "\n",
    "\n",
    "Let's characterize the region in $x$-space where this network's output indicates the first class (that is, $a_{1}^{2}$ is larger) or indicates the second class (that is, $a_{2}^{2}$ is larger). Your diagram from the previous part will be useful here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is the output value of the neural network in each of the following cases? Write your answer for $a_{i}^{2}$ as expressions, you can use powers of $e$, for example, $e^{* *} 2+1$; the exponents can be negative, $e^{* *}(-2)+1$.\n",
    "\n",
    "**Case 1) For $f\\left(z_{1}^{1}\\right)+f\\left(z_{2}^{1}\\right)+f\\left(z_{3}^{1}\\right)+f\\left(z_{4}^{1}\\right)=0$**\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "$a_{1}^{2}= \\frac{e^{z_{1}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \n",
    "\\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot 1 +f\\left(z_{2}^{1}\\right) \\cdot 1 +f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{1}{e^{2}+1} $\n",
    "\n",
    "$a_{2}^{2}= \\frac{e^{z_{2}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{e^{2}}{e^{2}+1} $\n",
    "\n",
    "Which class is predicted?\n",
    "\n",
    "\n",
    "class 2 is predicted\n",
    "\n",
    "\\end{exercise}\n",
    "\n",
    "**Case 2) For $f\\left(z_{1}^{1}\\right)+f\\left(z_{2}^{1}\\right)+f\\left(z_{3}^{1}\\right)+f\\left(z_{4}^{1}\\right)=1$**\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "$a_{1}^{2}= \\frac{e^{z_{1}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \n",
    "\\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot 1 +f\\left(z_{2}^{1}\\right) \\cdot 1 +f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{e}{e+e}=\\frac{1}{2}  $\n",
    "\n",
    "$a_{2}^{2}= \\frac{e^{z_{2}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{e}{e+e}=\\frac{1}{2} $\n",
    "\n",
    "Which class is predicted?\n",
    "\n",
    "depends on the criteria. Class is ambiguous.\n",
    "\n",
    "\\end{exercise}\n",
    "\n",
    "**Case 3) For $f\\left(z_{1}^{1}\\right)+f\\left(z_{2}^{1}\\right)+f\\left(z_{3}^{1}\\right)+f\\left(z_{4}^{1}\\right)=3$**\n",
    "\n",
    "\\begin{exercise}\n",
    "\n",
    "$a_{1}^{2}= \\frac{e^{z_{1}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \n",
    "\\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot 1 +f\\left(z_{2}^{1}\\right) \\cdot 1 +f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{e^{3}}{e^{3}+e^{-1}}  $\n",
    "\n",
    "$a_{2}^{2}= \\frac{e^{z_{2}^{2}}}{e^{z_{1}^{2}}+e^{z_{2}^{2}}} = \\frac{e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}}{e^{a_{1}^{1} w_{1,1}^{2}+a_{2}^{1} w_{2,1}^{2}+a_{3}^{1} w_{3,1}^{2}+a_{4}^{1} w_{4,1}^{2}+w_{0,1}^{2}}+e^{a_{1}^{1} w_{1,2}^{2}+a_{2}^{1} w_{2,2}^{2}+a_{3}^{1} w_{3,2}^{2}+a_{4}^{1} w_{4,2}^{2}+w_{0,2}^{2}}} = \\frac{e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}}{e^{f\\left(z_{1}^{1}\\right)\\cdot 1+f\\left(z_{2}^{1}\\right)\\cdot 1+f\\left(z_{3}^{1}\\right)\\cdot 1+f\\left(z_{4}^{1}\\right)\\cdot 1}+e^{f\\left(z_{1}^{1}\\right)\\cdot (-1)+f\\left(z_{2}^{1}\\right)\\cdot (-1)+f\\left(z_{3}^{1}\\right)\\cdot (-1)+f\\left(z_{1}^{1}\\right)\\cdot (-1)+2}} = \\frac{e^{-1}}{e^{3}+e^{-1}} $\n",
    "\n",
    "Which class is predicted?\n",
    "\n",
    "class 1 is predicted.\n",
    "\n",
    "\\end{exercise}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
