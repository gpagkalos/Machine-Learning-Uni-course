{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - Classification - Solvers - Performance_Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "In this home quiz you must do the following:\n",
    "\n",
    "1) Study the models presented here and their performance. This material will be part of QUIZ2.\n",
    "\n",
    "2) Run all the models presented here for the following wine dataset.\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      " 13  target                        178 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "#Let's import the data from sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "wine=load_wine()\n",
    "\n",
    "#Conver to pandas dataframe\n",
    "data=pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
    "\n",
    "#Check data with info function\n",
    "data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1 - Loading the iris data set\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# STEP 2 - Defining the predictors and target variable\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# STEP 3 - Splitting the data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=20052017)\n",
    "y_train, y_test = train_test_split(y, test_size=0.3, random_state=20052017)\n",
    "\n",
    "# STEP 4 - Importing and fitting the model\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "iris_BC = BaggingClassifier(n_estimators=100, max_features=3, random_state=0)\n",
    "BC = iris_BC.fit(X_train,y_train)\n",
    "\n",
    "# STEP 5 - Calculating the score of the model using the test data\n",
    "BC.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - CLASSIFICATION AND REGRESSION TREES (CART)\n",
    "- Set of supervised learning models used for problems involving classification and regression\n",
    "\n",
    "## Classification Tree\n",
    "- Sequence of if-else questions about individual features\n",
    "- Objective: Infer class labels\n",
    "- Able to capture non-linear relationships between features and labels\n",
    "- Don't require feature scaling. For example, it do not need standardization etc.\n",
    "\n",
    "## Decision regions \n",
    "- Decision region is the region in the feature space where all the instances are assigned to one class label.\n",
    "- For example, if result is of two class Pass or Fail. Then there will be 2 decision region. One is Pass region other is Fail region.\n",
    "\n",
    "## Decision boundary\n",
    "- It is the seperating boundary between two region.\n",
    "- In above example, decision boundary will be 33% (which is the passing marks)\n",
    "\n",
    "## Logistic regression vs classification tree\n",
    "- A classification tree divides the feature space into rectangular regions.\n",
    "- In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions.\n",
    "- In other word, decision boundary produced by logistic regression is linear (line) while the boundaries produced by the classification tree divide the feature space into rectangular regions (Not a line but boxes/region it divides two class).\n",
    "\n",
    "## Building block of Decision Tree \n",
    "- Root: No parent node, question giving rise to two children nodes.\n",
    "- Internal node: One parent node, question giving rise to two children nodes.\n",
    "- Lead: One parent node, no children node -> Prediction.\n",
    "\n",
    "## Classication-Tree Learning (Working) - \n",
    "- Nodes are grown recursively (based on last node).\n",
    "- At each node, split the data based on:\n",
    "1. feature f and split-point(sp) to maximize IG(Information gain from each node).\n",
    "2. If IG(node)= 0, declare the node a leaf.\n",
    "\n",
    "## Information Gain-\n",
    "- IG is a synonym for Kullback–Leibler divergence.\n",
    "- It is the amount of information gained about a random variable or signal from observing another random variable.\n",
    "- The term is sometimes used synonymously with mutual information, which is the conditional expected value of the Kullback–Leibler divergence.\n",
    "- KL divergance is the univariate probability distribution of one variable from the conditional distribution of this variable given the other one.\n",
    "\n",
    "## Criteria to measure the impurity of a node I(node):\n",
    "1. Variance (Regression) [Variance reduction of a node N is defined as the total reduction of the variance of the target variable x due to the split at this node]\n",
    "2. Gini impurity (Classification) [Measure of impurity. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset]\n",
    "3. Entropy (Classification) [Measure of purity. Information entropy is the average rate at which information is produced by a stochastic source of data]\n",
    "\n",
    "Note \n",
    "- Most of the time, the gini index and entropy lead to the same results.\n",
    "- The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn\n",
    "\n",
    "## Regression Tree Classification\n",
    "- Measurement are done through MSE (Mean Square error)\n",
    "- Information Gain is the MSE. So the target variable will have the Mean Square Error.\n",
    "- Regression trees tries to find the split that produce the leaf where in each leaf, the target value are an average of closest possible to the mean value of labels in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - BIAS VARIANCE TRADEOFF\n",
    "\n",
    "## Supervised Learning\n",
    "- y = f(x), f is the function which is unknown\n",
    "- Our model output will be that function\n",
    "- But that function may contains various type of error like noise\n",
    "\n",
    "## Goals of Supervised Learning\n",
    "- Find a model f1 that best approximates f: f1 ≈ f ()\n",
    "- f1 can be LogisticRegression, Decision Tree, Neural Network ...\n",
    "- Discard noise as much as possible.\n",
    "- End goal:f1 should acheive a low predictive error on unseen datasets.\n",
    "\n",
    "## Dificulties in Approximating f\n",
    "- Overtting: f1(x) fits the training set noise.\n",
    "- Undertting: f1 is not flexible enough to approximate f\n",
    "\n",
    "## Generalization error \n",
    "- Generalization Error of f1 : Does f1 generalize well on unseen data?\n",
    "- It can be decomposed as follows: Generalization Error of\n",
    "- f1 = bias + variance + irreducible error\n",
    "\n",
    "## Bias\n",
    "- Bias: error term that tells you, on average, how much f1 ≠ f.\n",
    "- High Bias lead to underfitting\n",
    "\n",
    "## Variance\n",
    "- Variance: tells you how much f is inconsistent over different training sets.\n",
    "- High Variance lead to overfitting\n",
    "\n",
    "- If we decrease Bias then Variance increase. Or Vice versa.\n",
    "\n",
    "## Model Complexity\n",
    "- Model Complexity: sets the flexibility of f1.\n",
    "- Example: Maximum tree depth, Minimum samples per leaf etc etc.\n",
    "\n",
    "## Bias Variance Tradeoff \n",
    "- It is the problem is in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n",
    "\n",
    "## Estimating the Generalization Error, Solution:\n",
    "- Split the data to training and test sets \n",
    "- Fit t1 to the training set\n",
    "- Evaluate the error of f1 on the unseen test set\n",
    "- Generalization error of f1 ≈ test set error of f1.\n",
    "\n",
    "## Better Model Evaluation with Cross-Validation\n",
    "- Test set should not be touched until we are confident about f1's performance.\n",
    "- Evaluating f1 on training set: biased estimate,f1 has already seen all training points.\n",
    "- Solution → K Cross-Validation (CV)\n",
    "\n",
    "## Diagnose Variance Problems\n",
    "- If f1 suffers from high variance: CV error of f1 > training set error of f1.\n",
    "- f1 is said to overfit the training set. To remedy overtting:\n",
    "- decrease model complexity\n",
    "- for ex: decrease max depth, increase min samples per leaf\n",
    "- gather more data\n",
    "\n",
    "## Diagnose Bias Problems\n",
    "- If f1 suffers from high bias: CV error of f1 ≈ training set error of f1 >> desired error.\n",
    "- f1 is said to underfit the training set. To remedy underfitting:\n",
    "- increase model complexity\n",
    "- for ex: increase max depth, decrease min samples per leaf\n",
    "- gather more relevant features\n",
    "\n",
    "## Limitations of CARTs\n",
    "- Classification: can only produce orthogonal decision boundaries.\n",
    "- Sensitive to small variations in the training set.\n",
    "- High variance: unconstrained CARTs may overt the training set.\n",
    "- Solution: ensemble learning.\n",
    "\n",
    "## Ensemble Learning\n",
    "- Train different models on the same dataset.\n",
    "- Let each model make its predictions.\n",
    "- Meta-model: aggregates predictions of individual models.\n",
    "- Final prediction: more robust and less prone to errors.\n",
    "- Best results: models are skillful in different ways.\n",
    "\n",
    "## Steps in Ensemble learning \n",
    "1. Training set is fed to different classifier like Decision tree, Logistic regression, KNN etc.\n",
    "2. Each classifier learn its parameter and make prediction\n",
    "3. Each prediction are fed into another model and that model make final prediction.\n",
    "4. That final model is known as ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - BAGGING AND RANDOM FOREST\n",
    "\n",
    "## Bagging\n",
    "- Bagging is an ensemble method involving training the same algorithm many times using different subsets sampled from the training data\n",
    "- In bagging, it uses same algorithm (only one algo is used)\n",
    "- However the model is not training on entire training set\n",
    "- Instead each model is trained on different subset of data\n",
    "- Bagging: Bootstrap Aggregation.\n",
    "- Uses a technique known as the bootstrap.\n",
    "- Reduces variance of individual models in the ensemble.\n",
    "- For example, suppose a training dataset contains 3 parts - a,b,c.\n",
    "- It create subset by method sample by replacement. For example aaa,aab,aba,acc,aca etc.\n",
    "- On this subset, the models are trained.\n",
    "\n",
    "## Bagging Classication\n",
    "- Aggregates predictions by majority voting (Final model is selected by voting).\n",
    "- BaggingClassifier in scikit-learn.\n",
    "\n",
    "## Bagging Regression\n",
    "- Aggregates predictions through averaging (Final model is selected by avergaing).\n",
    "- BaggingRegressor in scikit-learn.\n",
    "\n",
    "## Bagging limitations\n",
    "- Some instances may be sampled severaltimes for one model,\n",
    "- Other instances may not be sampled at all.\n",
    "\n",
    "## Out Of Bag (OOB) instances\n",
    "- On average,for each model, 63% ofthe training instances are sampled.\n",
    "- The remaining 37% constitute the OOB instances.\n",
    "- Since OOB instances are not seen by the model during training.\n",
    "- This can be used to estimate the performance of the model without the need of cross validation.\n",
    "- This technique is known as OOB evaluation\n",
    "\n",
    "## Random Forest\n",
    "- Another ensemble model\n",
    "- Base estimator: Decision Tree\n",
    "- Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "- d features are sampled at each node without replacement ( d < total number of features)\n",
    "\n",
    "## Random Forests Classication:\n",
    "- Aggregates predictions by majority voting\n",
    "- RandomForestClassifier in scikit-learn\n",
    "\n",
    "## Random Forests Regression:\n",
    "- Aggregates predictions through averaging\n",
    "- RandomForestRegressor in scikit-learn\n",
    "\n",
    "## Feature Importance\n",
    "- Tree-based methods: enable measuring the importance of each feature in prediction.\n",
    "- In sklearn :\n",
    "- how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "- accessed using the attribute feature_importance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 BOOSTING\n",
    "\n",
    "- Boosting refers to an ensemble method in which several models are trained sequentially with each model learning from the errors of its predecessors. \n",
    "- Boosting: Ensemble method combining several weak learners to form a strong learner.\n",
    "- Weak learner: Model doing slightly better than random guessing.\n",
    "- Example of weak learner: Decision stump (CART whose maximum depth is 1).\n",
    "- Train an ensemble of predictors sequentially.\n",
    "- Each predictor tries to correct its predecessor.\n",
    "- Most popular boosting methods: AdaBoost, Gradient Boosting.\n",
    "\n",
    "## Adaboost\n",
    "- Stands for Adaptive Boosting.\n",
    "- Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n",
    "- Achieved by changing the weights of training instances.\n",
    "- Each predictor is assigned a coefficient α.\n",
    "- α depends on the predictor's training error\n",
    "- Learning rate: 0 < η ≤ 1. It help to shrink coeeficient α. It is the tradeoff between η and number of estimator.\n",
    "- Smaller number of η should be compensiated by high number of estimator.\n",
    "\n",
    "## AdaBoost Classication\n",
    "- Weighted majority voting.\n",
    "- In sklearn: AdaBoostClassifier.\n",
    "\n",
    "## AdaBoost Regression\n",
    "- Weighted average.\n",
    "- In sklearn: AdaBoostRegressor.\n",
    "\n",
    "## Gradient Boosted Trees\n",
    "- Sequential correction of predecessor's errors.\n",
    "- Does not tweak the weights of training instances.\n",
    "- Fit each predictor is trained using its predecessor's residual errors as labels.\n",
    "- Gradient Boosted Trees: a CART is used as a base learner.\n",
    "\n",
    "## Gradient Boosted Regression:\n",
    "- y = y + ηr + ... + ηr\n",
    "- In sklearn: GradientBoostingRegressor .\n",
    "\n",
    "## Gradient Boosted Classication:\n",
    "- In sklearn: GradientBoostingClassifier .\n",
    "\n",
    "## Gradient Boosting: Cons\n",
    "- GB involves an exhaustive search procedure.\n",
    "- Each CART is trained to find the best split points and features.\n",
    "- May lead to CARTs using the same split points and maybe the same features.\n",
    "\n",
    "## Stochastic Gradient Boosting\n",
    "- Each tree is trained on a random subset of rows of the training data.\n",
    "- The sampled instances (40%-80% ofthe training set) are sampled without replacement.\n",
    "- Features are sampled (without replacement) when choosing split points.\n",
    "- Result: further ensemble diversity.\n",
    "- Effect: adding further variance to the ensemble oftrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5 - MODEL TUNING\n",
    "\n",
    "- The hyperparameters of a machine learning model are parameters that are not learned from data.\n",
    "- They should be set prior to fitting the model to the training set.\n",
    "\n",
    "## Parameters\n",
    "- learned from data\n",
    "- CART example: split-point of a node, split-feature of a node, ...\n",
    "\n",
    "## Hyperparameters\n",
    "- not learned from data, set prior to training\n",
    "- CART example: max_depth , min_samples_leaf , splitting criterion ...\n",
    "\n",
    "## What is hyperparameter tuning?\n",
    "- Problem: search for a set of optimal hyperparameters for a learning algorithm.\n",
    "- Solution: find a set of optimal hyperparameters that results in an optimal model.\n",
    "- Optimal model: yields an optimal score.\n",
    "- Score: in sklearn defaults to accuracy (classication) and R-squared (regression).\n",
    "- Cross validation is used to estimate the generalization performance.\n",
    "\n",
    "## Why tune hyperparameters?\n",
    "- In sklearn, a model's default hyperparameters are not optimal for all problems.\n",
    "- Hyperparameters should be tuned to obtain the best model performance.\n",
    "\n",
    "## Approaches to hyperparameter tuning\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization\n",
    "- GeneticAlgorithms etc.\n",
    "\n",
    "## Grid search cross validation\n",
    "- Manually set a grid of discrete hyperparameter values.\n",
    "- Set a metric for scoring model performance.\n",
    "- Search exhaustively through the grid.\n",
    "- For each set of hyperparameters, evaluate each model's CV score.\n",
    "- The optimal hyperparameters are those ofthe model achieving the best CV score.\n",
    "\n",
    "## Grid search cross validation: example\n",
    "- Hyperparameters grids:\n",
    "- max_depth = {2,3,4},\n",
    "- min_samples_leaf = {0.05, 0.1}\n",
    "- hyperparameter space = { (2,0.05) , (2,0.1) , (3,0.05), ... }\n",
    "- CV scores = { score , ... }\n",
    "- optimal hyperparameters = set of hyperparameters corresponding to the best CV score.\n",
    "\n",
    "Tuning a RF's Hyperparameters\n",
    "- Tuning is expensive\n",
    "\n",
    "## Hyperparameter tuning:\n",
    "- computationally expensive,\n",
    "- sometimes leads to very slight improvement,\n",
    "- Weight the impact oftuning on the whole project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # PART VI - PROBABILISTIC CLASSIFIERS\n",
    " \n",
    " ## Classification Problem\n",
    " \n",
    " * Given: $N$ labeled training examples $\\left\\{\\bf {x}_{n}, y_{n}\\right\\}_{n=1}^{N}, \\bf{x}_{n} \\in \\mathbb{R}^{D}, y_{n} \\in\\{0,1\\}$\n",
    "\n",
    "* $\\mathrm{X}: N \\times D$ feature matrix, $\\mathbf y: N \\times 1$ label vector\n",
    "\n",
    "* $y_{n}=1:$ positive example, $y_{n}=0:$ negative example\n",
    "\n",
    "* Goal: Learn a classifier (determine the parameters $\\omega$ of a function $h(\\bf x,\\omega)$) that is used to predict the binary label $y_{*}$ for a new input $\\bf x_{*}$\n",
    "\n",
    "* Often we don't just care about predicting the label $y$ for an example\n",
    "\n",
    "- Rather, we want to predict the label probabilities $P(y \\mid \\bf {x}_{n}, \\bf \\omega)$\n",
    "\n",
    "    - E.g., $P(y = +1 \\mid \\bf {x}_{n}, \\bf \\omega)$: the probability that the label is $+1$\n",
    "    \n",
    "    - In a sense, it's our confidence in the predicted label\n",
    "    \n",
    "- Probabilistic classification models allow us do that\n",
    "\n",
    "* Want a **probabilistic** model to be able to also predict the _*label probabilities*_\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{p\\left(y_{n}=1 | \\bf {x}_{n}, \\bf{\\omega}\\right)=\\mu_{n}} \\\\\n",
    "{p\\left(y_{n}=0 | \\bf {x}_{n}, \\bf{\\omega}\\right)=1-\\mu_{n}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "* $ \\mu_{n} \\in(0,1)$ is the probability of $y_{n}$ being 1\n",
    "\n",
    "* Note: Features $\\mathbf x_{n}$ assumed fixed (given). Only labels $y_{n}$ being modeled\n",
    "\n",
    "* $\\mathbf w$ is the model parameter (to be learned)\n",
    "\n",
    "* How do we define $\\mu_{n}$ (want it to be a function of $\\mathbf {\\omega}$ and input $\\bf x_{n}$ )?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Logistic regression\n",
    " \n",
    " \n",
    "\n",
    "- Often we don't just care about predicting the label $y$ for an example\n",
    "\n",
    "- Rather, we want to predict the label probabilities $P(y \\mid X, \\omega)$\n",
    "\n",
    "    - E.g., $P(y = +1 \\mid X, \\omega)$: the probability that the label is $+1$\n",
    "    \n",
    "    - In a sense, it's our confidence in the predicted label\n",
    "    \n",
    "- Probabilistic classification models allow us do that\n",
    "\n",
    "- Consider the following function $(y = -1/+1)$:\n",
    "\n",
    "$$P(y \\mid X, \\omega) = \\sigma(y\\omega^TX) = \\frac{1}{1 + \\exp(-y\\omega^TX)}$$\n",
    "\n",
    "\n",
    "- $\\sigma$ is the logistic function which maps all real number into $(0,1)$\n",
    "\n",
    "- This is the Logistic Regression model\n",
    "\n",
    "    - Misnomer: Logistic Regression is a classification model\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the decision boundary look like for Logistic Regression?\n",
    "\n",
    "- At the decision boundary labels $+1/-1$ becomes equiprobable\n",
    "\n",
    "\\begin{align*}\n",
    "P(y= + 1 \\mid x, \\omega) &= P(y = -1 \\mid x, \\omega)\\\\\n",
    "\\frac{1}{1+\\exp(-\\omega^TX)} &= \\frac{1}{1+\\exp(\\omega^TX)}\\\\\n",
    "\\exp(-\\omega^TX) &= \\exp(\\omega^TX)\\\\\n",
    "\\omega^TX&= 0\n",
    "\\end{align*}\n",
    "\n",
    "- The decision boundary is therefore linear $\\implies$ Logistic Regression is a linear classifier (note: it's possible to kernelize and make it nonlinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Solution\n",
    "\n",
    "- Goal: Want to estimate $\\omega$ from the data $D = \\{ (x_1, y_1), \\cdots, (x_m, y_m)\\}$\n",
    "\n",
    "\n",
    "- Log-likelihood:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log L(\\omega) = \\log P(D \\mid \\omega) = \\log P(Y \\mid X, \\omega) &= \\log\\prod\\limits^m_{n=1}P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum^m_{n=1}\\log P(y_n \\mid x_n, \\omega)\\\\\n",
    "&= \\sum^m_{n=1}\\log \\frac{1}{1+\\exp(-y_n\\omega^Tx_n)}\\\\\n",
    "&= \\sum^m_{n=1}-\\log\\left[1+\\exp(-y_n\\omega^Tx_n)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "- Maximum Likelihood Solution: \n",
    "\n",
    "$$\\hat{\\omega}_{MLE} = \\arg\\max_{\\omega}\\log L(\\omega)$$\n",
    "\n",
    "- No closed-form solution exists but we can do gradient descent on $\\omega$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\omega} \\log L(\\omega) &= \\sum^m_{n=1} -\\frac{1}{1 + \\exp\\left(-y_n\\omega^Tx_n\\right)}\\exp\\left(-y_n\\omega^Tx_n\\right)(-y_nx_n)\\\\\n",
    "&= \\sum^m_{n=1} \\frac{1}{1 + \\exp\\left(y_n\\omega^Tx_n\\right)}y_nx_n\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-a-Posteriori Solution\n",
    "\n",
    "- Let's assume a Gaussian prior distribution over the weight vector $\\omega$\n",
    "\n",
    "$$P(\\omega) = N(\\omega \\mid 0, \\lambda^{-1}I) = \\frac{1}{(2\\pi)^{D/2}}\\exp\\left( -\\frac{\\lambda}{2} \\omega^T\\omega \\right)$$\n",
    "\n",
    "- Maximum-a-Posteriori Solution: \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\omega}_{MAP} &= \\arg\\max_{\\omega} \\log P(\\omega \\mid D) \\\\\n",
    "&= \\arg\\max_{\\omega}\\{\\log P(\\omega) + \\log P(D \\mid \\omega) - \\underbrace{\\log P(D)}_{\\text{constant}} \\}&\\\\\n",
    "&= \\arg\\max_{\\omega}\\{\\log P(\\omega) + \\log P(D \\mid \\omega)\\}&\\\\\n",
    "&= \\arg\\max_{\\omega}\\bigg\\{ -\\frac{D}{2}\\log (2\\pi) - \\frac{\\lambda}{2}\\omega^T\\omega + \\sum\\limits^m_{n=1} - \\log\\left[1 + \\exp\\left(-y_n\\omega^Tx_n\\right)\\right] \\bigg\\}&\\\\\n",
    "&= \\arg\\min_{\\omega}\\sum\\limits^m_{n=1} \\log\\left[1 + \\exp\\left(-y_n\\omega^Tx_n\\right)\\right] + \\frac{\\lambda}{2}\\omega^T\\omega \\quad \\text{ (ignoring constants and changing max to min)}&\n",
    "\\end{align*}\n",
    "\n",
    "- BIG Lesson: MAP $= l_2$ norm regularization\n",
    "\n",
    "\n",
    "- No closed-form solution exists but we can do gradient descent on $\\omega$\n",
    "\n",
    "\n",
    "- See “[A comparison of numerical optimizers for logistic regression](http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf)” by Tom Minka on optimization techniques (gradient descent and others) for logistic regression (both MLE and MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes Classifier\n",
    "\n",
    "\n",
    "\n",
    "Let us use Bayes theorem, to express the classifier as\n",
    "\n",
    "$$\\hat{y} = \\mathrm{argmax}_y \\> p(y  \\mid  \\mathbf{X}) = \\mathrm{argmax}_y \\> \\frac{p( \\mathbf{X}  \\mid  y) p(y)}{p(\\mathbf{X})}.$$\n",
    "\n",
    "Note that the denominator is the normalizing term $p(\\mathbf{X})$ which does not depend on the value of the label $y$. As a result, we only need to worry about comparing the numerator across different values of $y$. Even if calculating the denominator turned out to be intractable, we could get away with ignoring it, so long as we could evaluate the numerator. Fortunately, even if we wanted to recover the normalizing constant, we could.  We can always recover the normalization term since $\\sum_y p(y  \\mid  \\mathbf{X}) = 1$.\n",
    "\n",
    "Now, let us focus on $p( \\mathbf{X}  \\mid  y)$ and assume that $X =\\{x_i\\}_{d=1}^d$. Using the chain rule of probability, we can express the term $p( \\mathbf{X}  \\mid  y)$ as\n",
    "\n",
    "$$p(x_1  \\mid y) \\cdot p(x_2  \\mid  x_1, y) \\cdot ... \\cdot p( x_d  \\mid  x_1, ..., x_{d-1}, y).$$\n",
    "\n",
    "By itself, this expression does not get us any further. We still must estimate roughly $2^d$ parameters. However, if we assume that *the features are conditionally independent of each other, given the label*, then suddenly we are in much better shape, as this term simplifies to $\\prod_i p(x_i  \\mid  y)$, giving us the predictor\n",
    "\n",
    "$$\\hat{y} = \\mathrm{argmax}_y \\> \\prod_{i=1}^d p(x_i  \\mid  y) p(y).$$\n",
    "\n",
    "If we can estimate $p(x_i=1  \\mid  y)$ for every $i$ and $y$, and save its value in $P_{xy}[i, y]$, here $P_{xy}$ is a $d\\times n$ matrix with $n$ being the number of classes and $y\\in\\{1, \\ldots, n\\}$, then we can also use this to estimate $p(x_i = 0 \\mid y)$, i.e.,\n",
    "\n",
    "$$ \n",
    "p(x_i = t_i \\mid y) = \n",
    "\\begin{cases}\n",
    "    P_{xy}[i, y] & \\text{for } t_i=1 ;\\\\\n",
    "    1 - P_{xy}[i, y] & \\text{for } t_i = 0 .\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In addition, we estimate $p(y)$ for every $y$ and save it in $P_y[y]$, with $P_y$ a $n$-length vector. Then, for any new example $\\mathbf t = (t_1, t_2, \\ldots, t_d)$, we could compute\n",
    "\n",
    "$$\\begin{aligned}\\hat{y} &= \\mathrm{argmax}_ y \\ p(y)\\prod_{i=1}^d   p(x_t = t_i \\mid y) \\\\ &= \\mathrm{argmax}_y \\ P_y[y]\\prod_{i=1}^d \\ P_{xy}[i, y]^{t_i}\\, \\left(1 - P_{xy}[i, y]\\right)^{1-t_i}\\end{aligned}$$\n",
    ":eqlabel:`eq_naive_bayes_estimation`\n",
    "\n",
    "for any $y$. So our assumption of conditional independence has taken the complexity of our model from an exponential dependence on the number of features $\\mathcal{O}(2^dn)$ to a linear dependence, which is $\\mathcal{O}(dn)$.\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "The problem now is that we do not know $P_{xy}$ and $P_y$. So we need to estimate their values given some training data first. This is *training* the model. Estimating $P_y$ is not too hard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Classification and Regression Trees (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.97\n",
      "0.21136680421940887\n",
      "Test set RMSE of dt: 0.21\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFICATION\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2,stratify=y,random_state=1)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'criterion' to 'gini'\n",
    "dt = DecisionTreeClassifier(criterion= 'gini', random_state=1)\n",
    "\n",
    "# Instantiate dt, set 'entropy' as the information criterion\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "\n",
    "# Most of the time, the gini index and entropy lead to the same results.\n",
    "# The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate test-set accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))\n",
    "\n",
    "# REGRESSION\n",
    "# Import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Split data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "# 0.1 implies Atleast 10% of the training data\n",
    "\n",
    "# Fit 'dt' to the training-set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test-set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test-set MSE\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test-set RMSE\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(rmse_dt)\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE: 0.08\n",
      "Train MSE: 0.03\n",
      "Test MSE: 0.03\n",
      "CV RMSE: 0.28\n"
     ]
    }
   ],
   "source": [
    "#K-Fold CV in regression\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate decision tree regressor and assign it to 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "# Evaluate the list of MSE ontained by 10-fold CV\n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv= 10, scoring= 'neg_mean_squared_error' , n_jobs = -1)\n",
    "\n",
    "# Fit 'dt' to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of training set\n",
    "y_predict_train = dt.predict(X_train)\n",
    "y_predict_train\n",
    "\n",
    "# Predict the labels of test set\n",
    "y_predict_test = dt.predict(X_test)\n",
    "\n",
    "# CV MSE\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean()))\n",
    "\n",
    "# Training set MSE\n",
    "print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "\n",
    "# Test set MSE\n",
    "print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))\n",
    "\n",
    "# Suppose CV MSE = 20.51, Train MSE = 15.30 and Test MSE = 20.92\n",
    "# Train MSE < CV MSE.\n",
    "# Suggested that model is overfit and is suffering from high variance.\n",
    "# CV MSE and Test MSE are roughly equal\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV.mean())**(1/2)\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.18\n",
      "Logistic Regression : 0.978\n",
      "K Nearest Neighbours : 0.978\n",
      "Classification Tree : 0.956\n",
      "Logistic Regression : 0.978\n",
      "K Nearest Neighbours : 0.978\n",
      "Classification Tree : 0.956\n",
      "Voting Classifier: 0.978\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_predict_train))**(1/2)\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "\n",
    "# Suppose, RMSE_CV = 5.14, RMSE_train = 5.15 and baseline_RMSE = 5.1\n",
    "# RMSE_CV < RMSE_train means dt suffers from high bias because RMSE_CV ≈ RMSE_train and both scores are greater than baseline_RMSE.\n",
    "# dt is indeed underfitting the training set as the model is too constrained to capture the nonlinear dependencies between features and labels\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "# Import functions to compute accuracy and split data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models, including VotingClassifier meta-model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state= SEED)\n",
    "\n",
    "# Instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN(n_neighbors=27)\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n",
    "\n",
    "# Iterate over the defined list of tuples containing the classifiers\n",
    "for clf_name, clf in classifiers:\n",
    "  \n",
    "  #fit clf to the training set\n",
    "  clf.fit(X_train, y_train)\n",
    "  \n",
    "  # Predict the labels of the test set\n",
    "  y_pred = clf.predict(X_test)\n",
    "  \n",
    "  # Evaluate the accuracy of clf on the test set\n",
    "  print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "  \n",
    "# OR \n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)   \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))\n",
    "\n",
    "# Instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "# Evaluate the test-set accuracy of 'vc'\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy_score(y_test, y_pred)))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Bagging and Random Forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.978\n",
      "Test set accuracy: 0.978\n",
      "OOB accuracy: 0.905\n",
      "Test set RMSE of rf: 0.17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BAGGING CLASSIFICATION\n",
    "# Import models and utility functions\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "\n",
    "# OOB EVALUATION IN SKLEARN\n",
    "# Import models and split utility function\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, stratify= y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'; set oob_score= True\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the traing set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Extract the OOB accuracy from 'bc'\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Print test set accuracy\n",
    "print('Test set accuracy: {:.3f}'.format(test_accuracy))\n",
    "\n",
    "# Print OOB accuracy\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))\n",
    "# The difference between test and oob accuracy will be minimal which proved that we don't need cross validation to check the model accuracy\n",
    "\n",
    "\n",
    "# RANDOM FOREST REGRESSOR\n",
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# FEATURE IMPORTANCE in sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "X_train = pd.DataFrame(X_train)\n",
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
    "\n",
    "#importances_rf = pd.DataFrame(rf.feature_importances_,\n",
    "#                                   index = X_train.columns,\n",
    "#                                    columns=['importance']).sort_values('importance',  \n",
    "#ascending=False)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind= 'barh', color= 'lightgreen'); \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "   petal length (cm)  petal width (cm)\n",
      "0                1.4               0.2\n",
      "1                1.4               0.2\n",
      "2                1.3               0.2\n",
      "3                1.5               0.2\n",
      "4                1.4               0.2\n",
      "\n",
      "The classes in this data are [0 1 2]\n",
      "Training set are 105 samples  and Test set are 45 samples\n",
      "\n",
      "After standardizing our features,data looks like as follows:\n",
      "\n",
      "   petal length (cm)  petal width (cm)\n",
      "0          -0.182950         -0.293181\n",
      "1           0.930661          0.737246\n",
      "2           1.042022          1.638870\n",
      "3           0.652258          0.350836\n",
      "4           1.097702          0.737246\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 95.24%\n",
      "The test accuracy is 93.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This class uses AdaBoost on Iris Dataset and Visiualize the data before and after prediction \n",
    "\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# Import data and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt \n",
    "pylab.rcParams['figure.figsize'] = (10, 6)\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    ## Load the iris data \n",
    "    iris = datasets.load_iris()\n",
    "    X_train, X_test, y_train, y_test,iris_df, X,y=get_data(iris)\n",
    "    X_train_std,X_test_std=scale_data(X_train,X_test,iris_df)\n",
    "    show_data(y_test,X,y)\n",
    "    classification=Adaboost(X_train_std,y_train, X_test_std, y_test)\n",
    "    classification.perform_adaboost(X_train_std, y_train, X_test_std, y_test)\n",
    "    \n",
    "def get_data(iris):\n",
    "# Only petal length and petal width considered\n",
    "    X = iris.data[:, [2, 3]]\n",
    "    \n",
    "    y = iris.target\n",
    "    \n",
    "# Place the iris data into a pandas dataframe\n",
    "    iris_df = pd.DataFrame(iris.data[:, [2, 3]], columns=iris.feature_names[2:])\n",
    "\n",
    "# View the data\n",
    "    print(iris_df.head())\n",
    "\n",
    "# Print the classes of the dataset\n",
    "    print('\\n' + 'The classes in this data are ' + str(np.unique(y)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
    "\n",
    "    print('Training set are {} samples  and Test set are {} samples'.format(\n",
    "    X_train.shape[0], X_test.shape[0]))\n",
    "    print()\n",
    "    return(X_train, X_test, y_train, y_test,iris_df, X,y)\n",
    "##scale the training data before training\n",
    "def scale_data(X_train,X_test,iris_df):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    print('After standardizing our features,data looks like as follows:\\n')\n",
    "    print(pd.DataFrame(X_train_std, columns=iris_df.columns).head())\n",
    "    return(X_train_std,X_test_std)\n",
    "##visualization of the data before training     \n",
    "def show_data(y_test,X,y):\n",
    "    ##There are 3 classes\n",
    "    markers = ('s', 'x', 'o')\n",
    "    colors = ('red', 'blue', 'green')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y_test))])\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "            plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                   c=cmap(idx), marker=markers[idx], label=cl)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.show()\n",
    "##Adaboost Class\n",
    "class Adaboost(object):\n",
    "   \n",
    "    def __init__(self,X_train_std,y_train,X_test_std, y_test):\n",
    "        self.X_train_std=X_train_std\n",
    "        self.y_train=y_train\n",
    "        self.X_test_std=X_test_std\n",
    "        self.y_test=y_test\n",
    "    def perform_adaboost(self,X_train_std,y_train,X_test_std, y_test): ##perform adaboost\n",
    "     \n",
    "        ada = AdaBoostClassifier(n_estimators=10)\n",
    "        ada.fit(X_train_std, y_train)\n",
    "        train_score=cross_val_score(ada,X_train_std, y_train)\n",
    "        print('The training accuracy is {:.2f}%'.format(train_score.mean()*100))\n",
    "        test_score=cross_val_score(ada,X_test_std, y_test)\n",
    "        print('The test accuracy is {:.2f}%'.format(test_score.mean()*100))\n",
    "        X=X_test_std\n",
    "        y=y_test\n",
    "        resolution=0.01\n",
    "        #Z = svm.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "        markers = ('s', 'x', 'o', '^', 'v')\n",
    "        colors = ('red', 'blue', 'green', 'gray', 'cyan')\n",
    "        cmap = ListedColormap(colors[:len(np.unique(y_test))])\n",
    "        X=X_test_std\n",
    "        y=y_test    \n",
    "        # plot the decision surface\n",
    "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "        Z = ada.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.5, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)\n",
    "        plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9473684210526315\n",
      "ROC AUC score: 0.987\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "#training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "#applying Adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "classifier=AdaBoostClassifier(n_estimators=30,learning_rate=1)\n",
    "adaboost=classifier.fit(X_train,y_train)\n",
    "y_pred=adaboost.predict(X_test)\n",
    "\n",
    "#calculating the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "# Evaluate test-set roc_auc_score\n",
    "#roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
    "\n",
    "\n",
    "# Print adb_clf_roc_auc_score\n",
    "print('ROC AUC score: {:.3f}'.format(roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoost Classication with Decision Tree in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# Instantiate an AdaBoost classifier 'adab_clf'\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "# Fit 'adb_clf' to the training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "# Once the classifier adb_clf is trained, call the .predict_proba() method by passing X_test as a parameter \n",
    "# Extract these probabilities by slicing all the values in the second column as follows\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test-set roc_auc_score\n",
    "roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
    "\n",
    "#adb_clf_roc_auc_score= roc_auc_score(y_test, y_pred_proba,multi_class='ovr' )\n",
    "\n",
    "# Print adb_clf_roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 0.18\n",
      "Test set RMSE: 0.19\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor 'gbt'\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "# Fit 'gbt' to the training set\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n",
    "\n",
    "\n",
    "# Stochastic Gradient Boosting in sklearn\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a stochastic GradientBoostingRegressor 'sgbt'\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "# 0.8 refers to sample 80% of datafor training\n",
    "# 0.2 refers to each tree uses 20% of the available features to perform best split\n",
    "\n",
    "# Fit 'sgbt' to the training set\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "# Evaluate test set RMSE 'rmse_test'\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print 'rmse_test'\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Model Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the hyperparameters of a CART\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set seed to 1 for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt'\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Print out 'dt's hyperparameters\n",
    "print(dt.get_params())\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of hyperparameters 'params_dt'\n",
    "params_dt = {\n",
    "              'max_depth': [3, 4,5, 6],\n",
    "              'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "              'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "            }\n",
    "\n",
    "# Instantiate a 10-fold CV grid search object 'grid_dt'\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "# Fit 'grid_dt' to the training data\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_dt'\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best CV score from 'grid_dt'\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy'.format(best_CV_score))\n",
    "\n",
    "# Extract best model from 'grid_dt'\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test,y_test)\n",
    "\n",
    "# Print test set accuracy\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))\n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "#y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba,multi_class='ovr')\n",
    "#roc_auc_score(y_test, y_pred_proba,multi_class='ovr')\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(roc_auc_score(y_test, y_pred_proba,multi_class='ovr')))\n",
    "\n",
    "\n",
    "# Inspecting RF Hyperparameters in sklearn\n",
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate a random forests regressor 'rf'\n",
    "rf = RandomForestRegressor(random_state= SEED)\n",
    "\n",
    "# Inspect rf' s hyperparameters\n",
    "rf.get_params()\n",
    "\n",
    "# Basic imports\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameter 'params_rf'\n",
    "params_rf = {\n",
    "                'n_estimators': [300, 400, 500],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'min_samples_leaf': [0.1, 0.2],\n",
    "                'max_features': ['log2','sqrt']\n",
    "            }\n",
    "\n",
    "# Instantiate 'grid_rf'\n",
    "grid_rf = GridSearchCV(estimator=rf,param_grid=params_rf, cv=3, scoring= 'neg_mean_squared_error',verbose=1, n_jobs=-1)\n",
    "\n",
    "# Searching for the best hyperparameters\n",
    "# Fit 'grid_rf' to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_rf'\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best model from 'grid_rf'\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 - Performance Metrics for Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve \n",
    "Scikit Learn has an easy way to create ROC curve and calculate the area under the ROC curve. First off, let’s start with a classifier like Logistic Regression and let it predict all the probabilities (thresholds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "from sklearn import datasets\n",
    " \n",
    "iris = datasets.load_iris()\n",
    " \n",
    "# Get only the setosa and versicolor data\n",
    "iris_data = iris.data[0:100,:]\n",
    "iris_target = iris.target[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a classifier - choose logistic regression\n",
    "# split the data into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split (iris_data[:,0], iris_target)\n",
    " \n",
    "# Model the data using Logistic Regression\n",
    "from sklearn import linear_model\n",
    " \n",
    "model = linear_model.LogisticRegression(C=1e5, solver='lbfgs')\n",
    "model.fit(iris_data[:,0].reshape(-1,1), iris_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Use roc_curve function to create the True Positive Rate and False positive Rate.\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "probabilities = model.predict_proba(X_test.reshape(-1,1))[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],color=\"black\",linestyle=\"--\")\n",
    " \n",
    "plt.xlabel(\"False Positive Rate - FPR\")\n",
    "plt.ylabel(\"True Positive Rate - TPR \")\n",
    "plt.title(\"Receiver Operating Characteristics - ROC Curve\")\n",
    "plt.text(0.6,0.5,\"Baseline\")\n",
    "plt.text(0.3,0.8,\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b0c21c83c956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# computes the area under the ROC curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "# computes the area under the ROC curve\n",
    "roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "X_test =np.array(X_test).reshape(-1,1)\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#dataset\n",
    "iris = pd.read_csv(\"iris.csv\") # the iris dataset is now a Pandas DataFrame\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure\n",
    "sns.jointplot(x=\"sepal_length\", y=\"sepal_width\", data=iris, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use seaborn's FacetGrid to color the scatterplot by species\n",
    "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
    "   .map(plt.scatter, \"sepal_length\", \"sepal_width\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Next is a visual based on probability density , called kernel density plots. (KD Plots)\n",
    "\n",
    "# A final seaborn plot useful for looking at univariate relations is the kdeplot,\n",
    "# which creates and visualizes a kernel density estimate of the underlying feature\n",
    "sns.FacetGrid(iris, hue=\"species\", size=6) \\\n",
    "   .map(sns.kdeplot, \"petal_length\") \\\n",
    "   .add_legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the data for training and testing\n",
    "\n",
    "#Once we have understood what the dataset is about, we can start training a model based on the algorithms. Here, we will be implementing some of the commonly used algorithms in machine learning. Let us start by training our model with some of the samples. We will be using an inbuilt library called ‘train_test_split’ which divides our data set into a ratio of 80:20.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seperating the data into dependent and independent variables\n",
    "X = iris.iloc[:, :-1].values\n",
    "y = iris.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complement Naive Bayes\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "classifier = ComplementNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    ComplementNB(),               \n",
    "                  ]\n",
    " \n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    " \n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, 11]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "    print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "573.2px",
    "left": "421px",
    "top": "254.306px",
    "width": "295.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
